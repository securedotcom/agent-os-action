"""
Performance and cost validation tests
"""

import os
import sys
import time
from pathlib import Path

import pytest

# Add scripts directory to path
scripts_dir = Path(__file__).parent.parent.parent / "scripts"
sys.path.insert(0, str(scripts_dir))


class TestPerformance:
    """Test performance and cost optimizations"""

    @pytest.mark.skip("Requires live API calls and costs money")
    def test_foundation_sec_cost_savings(self, tmp_path):
        """Measure cost savings with Foundation-Sec-8B vs Claude"""
        from run_ai_audit import run_audit

        # Create test repository
        repo_dir = tmp_path / "test_repo"
        repo_dir.mkdir()
        (repo_dir / "app.py").write_text(
            """
def process_payment(amount, user_input):
    query = f"SELECT * FROM payments WHERE amount = {amount} AND user = '{user_input}'"
    return db.execute(query)

def display_message(msg):
    return eval(msg)
        """
        )

        # Run with Claude
        config_claude = {
            "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY"),
            "ai_provider": "anthropic",
            "max_files": 5,
            "max_tokens": 2000,
        }

        results_claude = run_audit(str(repo_dir), config_claude, "audit")
        cost_claude = results_claude["metrics"]["cost_usd"]

        # Run with Foundation-Sec
        config_foundation = {
            "ai_provider": "foundation-sec",
            "foundation_sec_enabled": True,
            "max_files": 5,
            "max_tokens": 2000,
        }

        results_foundation = run_audit(str(repo_dir), config_foundation, "audit")
        cost_foundation = results_foundation["metrics"]["cost_usd"]

        # Verify savings
        assert cost_foundation == 0.0
        assert cost_claude > 0.0

        savings_pct = (1 - cost_foundation / cost_claude) * 100 if cost_claude > 0 else 0
        assert savings_pct >= 75  # At least 75% savings

        print(f"Cost savings: {savings_pct:.2f}%")
        print(f"Claude cost: ${cost_claude:.4f}")
        print(f"Foundation-Sec cost: ${cost_foundation:.4f}")

    def test_cost_estimation_accuracy(self):
        """Test that cost estimation is accurate"""
        from run_ai_audit import estimate_call_cost

        # Test various providers
        providers_and_costs = [
            ("anthropic", "claude-sonnet-4-5-20250929", 1000000, 1000000, 18.0),  # $3 + $15
            ("foundation-sec", None, 1000000, 1000000, 0.0),
            ("ollama", None, 1000000, 1000000, 0.0),
        ]

        for provider, model, input_tokens, output_tokens, expected_cost in providers_and_costs:
            cost = estimate_call_cost(input_tokens, output_tokens, provider, model)
            assert abs(cost - expected_cost) < 0.01, f"Cost mismatch for {provider}"

    @pytest.mark.skip("Requires live implementation")
    def test_heuristic_filtering_performance(self, tmp_path):
        """Verify heuristic pre-filtering improves performance"""
        from run_ai_audit import run_audit

        # Create repository with many files (some safe, some vulnerable)
        repo_dir = tmp_path / "large_repo"
        repo_dir.mkdir()

        # Create safe files (should be filtered by heuristics)
        for i in range(20):
            (repo_dir / f"safe_{i}.py").write_text(
                f'''
def safe_function_{i}(data):
    """Safe function {i}"""
    result = data.strip().lower()
    return result.capitalize()
            '''
            )

        # Create vulnerable files (should pass heuristics)
        for i in range(5):
            (repo_dir / f"vuln_{i}.py").write_text(
                f"""
def vulnerable_{i}(user_input):
    query = f"SELECT * FROM table WHERE id = '{{user_input}}'"
    return eval(user_input)
            """
            )

        # Run without heuristics
        start_time = time.time()
        config_no_heuristics = {
            "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY"),
            "enable_heuristic_filtering": False,
            "max_files": 25,
            "max_tokens": 2000,
        }
        results_no_heuristics = run_audit(str(repo_dir), config_no_heuristics, "audit")
        time_without = time.time() - start_time

        # Run with heuristics
        start_time = time.time()
        config_with_heuristics = {
            "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY"),
            "enable_heuristic_filtering": True,
            "max_files": 25,
            "max_tokens": 2000,
        }
        results_with_heuristics = run_audit(str(repo_dir), config_with_heuristics, "audit")
        time_with = time.time() - start_time

        # Should be faster with heuristics
        assert time_with < time_without

        # Should have fewer API calls
        assert (
            results_with_heuristics["metrics"]["files_reviewed"] <= results_no_heuristics["metrics"]["files_reviewed"]
        )

        print(f"Time without heuristics: {time_without:.2f}s")
        print(f"Time with heuristics: {time_with:.2f}s")
        print(f"Speedup: {(time_without / time_with):.2f}x")

    @pytest.mark.skip("Requires Docker")
    def test_sandbox_validation_performance(self, tmp_path):
        """Test sandbox validation completes in reasonable time"""
        from sandbox_validator import SandboxValidator

        validator = SandboxValidator()

        # Create test exploit
        exploit_code = """
import subprocess
import time

# Simulate some work
time.sleep(0.1)

result = subprocess.run(['echo', 'test'], capture_output=True)
print(result.stdout.decode())
        """

        finding = {"vulnerability": "Command Injection", "severity": "high"}

        # Time the validation
        start_time = time.time()
        validator.validate_exploit(exploit_code, finding)
        duration = time.time() - start_time

        # Should complete in < 10 seconds
        assert duration < 10, f"Sandbox validation took {duration:.2f}s (limit: 10s)"

        print(f"Sandbox validation duration: {duration:.2f}s")

    def test_file_selection_performance(self, tmp_path):
        """Test that file selection is fast even with many files"""
        from run_ai_audit import select_files_for_review

        # Create many files
        repo_dir = tmp_path / "large_repo"
        repo_dir.mkdir()

        for i in range(1000):
            (repo_dir / f"file_{i}.py").write_text(f"def func_{i}(): pass\n")

        config = {
            "max_files": "50",
            "max_file_size": "50000",
            "include_paths": "",
            "exclude_paths": "",
        }

        # Time file selection
        start_time = time.time()
        files = select_files_for_review(str(repo_dir), config)
        duration = time.time() - start_time

        # Should complete in < 5 seconds
        assert duration < 5, f"File selection took {duration:.2f}s (limit: 5s)"

        # Should respect max_files limit
        assert len(files) <= 50

        print(f"File selection duration: {duration:.2f}s for 1000 files")

    @pytest.mark.skip("Requires live API")
    def test_threat_model_generation_performance(self, tmp_path):
        """Test threat model generation performance"""
        from threat_model_generator import ThreatModelGenerator

        # Create test repository
        repo_dir = tmp_path / "test_repo"
        repo_dir.mkdir()
        (repo_dir / "app.py").write_text("def main(): pass")
        (repo_dir / "README.md").write_text("# Test App")

        api_key = os.getenv("ANTHROPIC_API_KEY")
        generator = ThreatModelGenerator(api_key)

        # Time repository analysis
        start_time = time.time()
        repo_context = generator.analyze_repository(str(repo_dir))
        analysis_duration = time.time() - start_time

        # Should be fast (< 1 second for small repo)
        assert analysis_duration < 1.0

        # Time threat model generation
        start_time = time.time()
        generator.generate_threat_model(repo_context)
        generation_duration = time.time() - start_time

        # Should complete in reasonable time (< 30 seconds)
        assert generation_duration < 30.0

        print(f"Repository analysis: {analysis_duration:.2f}s")
        print(f"Threat model generation: {generation_duration:.2f}s")

    def test_metrics_recording_performance(self):
        """Test that metrics recording doesn't slow down execution"""
        from run_ai_audit import ReviewMetrics

        metrics = ReviewMetrics()

        # Record many events
        start_time = time.time()
        for _i in range(1000):
            metrics.record_file(50)
            metrics.record_llm_call(1000, 500, "anthropic")
        duration = time.time() - start_time

        # Should be very fast (< 100ms for 1000 events)
        assert duration < 0.1

        print(f"Recorded 1000 events in {duration * 1000:.2f}ms")


class TestScalability:
    """Test system scales well with larger codebases"""

    def test_memory_usage_for_large_repo(self, tmp_path):
        """Test memory usage remains reasonable for large repositories"""
        import os

        import psutil

        # Get current process
        process = psutil.Process(os.getpid())

        # Measure initial memory
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Create large repository
        repo_dir = tmp_path / "large_repo"
        repo_dir.mkdir()

        for i in range(100):
            (repo_dir / f"file_{i}.py").write_text("def func(): pass\n" * 100)

        from run_ai_audit import select_files_for_review

        config = {
            "max_files": "100",
            "max_file_size": "50000",
            "include_paths": "",
            "exclude_paths": "",
        }

        select_files_for_review(str(repo_dir), config)

        # Measure final memory
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory

        # Memory increase should be reasonable (< 100 MB)
        assert memory_increase < 100

        print(f"Memory increase: {memory_increase:.2f} MB")

    def test_token_counting_performance(self, tmp_path):
        """Test token counting is fast"""
        from run_ai_audit import estimate_tokens

        # Large text
        large_text = "This is a test sentence. " * 10000

        # Time token estimation
        start_time = time.time()
        estimate_tokens(large_text)
        duration = time.time() - start_time

        # Should be fast (< 1 second)
        assert duration < 1.0

        print(f"Token counting duration: {duration * 1000:.2f}ms for {len(large_text)} chars")

    def test_concurrent_file_processing(self, tmp_path):
        """Test that processing multiple files doesn't cause issues"""
        from run_ai_audit import select_files_for_review

        # Create many files
        repo_dir = tmp_path / "large_repo"
        repo_dir.mkdir()

        for i in range(50):
            (repo_dir / f"file_{i}.py").write_text(
                f'''
def function_{i}(param):
    """Function {i}"""
    result = param * 2
    return result
            '''
            )

        config = {
            "max_files": "50",
            "max_file_size": "50000",
            "include_paths": "",
            "exclude_paths": "",
        }

        # Process files
        start_time = time.time()
        files = select_files_for_review(str(repo_dir), config)
        duration = time.time() - start_time

        # Should handle 50 files quickly
        assert duration < 5.0
        assert len(files) <= 50

        print(f"Processed {len(files)} files in {duration:.2f}s")


class TestCostOptimization:
    """Test cost optimization features"""

    def test_cost_circuit_breaker_prevents_overspend(self):
        """Test circuit breaker prevents exceeding budget"""
        from run_ai_audit import CostCircuitBreaker, CostLimitExceeded

        breaker = CostCircuitBreaker(cost_limit_usd=1.0)

        # Record increasing costs
        breaker.record_actual_cost(0.3)
        breaker.record_actual_cost(0.3)
        breaker.record_actual_cost(0.3)

        # Should now be near limit (0.9 out of 1.0)
        # Next large operation should fail
        with pytest.raises(CostLimitExceeded):
            breaker.check_before_call(0.5, "anthropic")

    @pytest.mark.skip(reason="CostCircuitBreaker warning threshold tracking is not implemented - feature works but warnings are printed, not tracked")
    def test_cost_warnings_at_thresholds(self):
        """Test that cost warnings are issued at threshold percentages"""
        from run_ai_audit import CostCircuitBreaker

        breaker = CostCircuitBreaker(cost_limit_usd=1.0)

        # Should warn at 50%, 75%, 90%
        breaker.record_actual_cost(0.5)  # 50%
        assert 50 in breaker.warned_thresholds

        breaker.record_actual_cost(0.25)  # 75%
        assert 75 in breaker.warned_thresholds

        breaker.record_actual_cost(0.15)  # 90%
        assert 90 in breaker.warned_thresholds

    def test_foundation_sec_eliminates_api_costs(self):
        """Test that Foundation-Sec has zero API costs"""
        from run_ai_audit import ReviewMetrics

        metrics = ReviewMetrics()

        # Simulate many calls with Foundation-Sec
        for _i in range(100):
            metrics.record_llm_call(1000, 500, "foundation-sec")

        output = metrics.finalize()

        # Cost should still be zero
        assert output["cost_usd"] == 0.0

    def test_token_limit_prevents_excessive_usage(self):
        """Test that max_tokens setting limits usage"""
        from run_ai_audit import estimate_tokens

        config = {"max_tokens": "1000"}

        large_text = "This is a test. " * 1000
        token_count = estimate_tokens(large_text)

        int(config["max_tokens"])

        # In practice, text should be truncated before sending
        # This test verifies token counting works
        assert token_count > 0


class TestResourceUtilization:
    """Test resource utilization is efficient"""

    def test_file_reading_is_efficient(self, tmp_path):
        """Test that file reading doesn't use excessive memory"""
        import os

        import psutil

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024

        # Create large file
        large_file = tmp_path / "large.py"
        large_file.write_text("x = 1\n" * 10000)

        # Read file
        from run_ai_audit import read_file_safe

        read_file_safe(str(large_file))

        final_memory = process.memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory

        # Memory increase should be reasonable
        assert memory_increase < 50  # Less than 50 MB

    def test_json_serialization_is_fast(self):
        """Test JSON serialization of results is fast"""
        import json

        from run_ai_audit import ReviewMetrics

        metrics = ReviewMetrics()

        # Create many findings
        for _i in range(100):
            metrics.record_file(50)

        output = metrics.finalize()

        # Time JSON serialization
        start_time = time.time()
        json_str = json.dumps(output, indent=2)
        duration = time.time() - start_time

        # Should be fast
        assert duration < 0.1

        print(f"JSON serialization: {duration * 1000:.2f}ms for {len(json_str)} bytes")

    def test_sarif_generation_is_efficient(self):
        """Test SARIF generation is efficient"""
        from run_ai_audit import generate_sarif_output

        # Create many findings
        findings = []
        for i in range(100):
            findings.append(
                {
                    "category": "security",
                    "severity": "medium",
                    "title": f"Issue {i}",
                    "description": f"Description {i}",
                    "file": f"file_{i}.py",
                    "line": i,
                    "code": f"code line {i}",
                    "recommendation": f"Fix {i}",
                }
            )

        # Time SARIF generation
        start_time = time.time()
        sarif = generate_sarif_output(findings, "/test/repo")
        duration = time.time() - start_time

        # Should be fast
        assert duration < 1.0

        # Should have all findings
        assert len(sarif["runs"][0]["results"]) == 100

        print(f"SARIF generation: {duration * 1000:.2f}ms for 100 findings")
