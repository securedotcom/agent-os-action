#!/usr/bin/env python3
"""
Fixed Argus Security Pipeline - Skipping Foundation-Sec and Claude
Uses Cursor AI for AI analysis, all other components working

Components:
1-5: Deterministic scanners (TruffleHog, Gitleaks, Semgrep, Trivy, Checkov)
6: Normalization + Deduplication
7: Cursor AI Analysis (skipping Foundation-Sec and Claude)
8-15: All other components
"""

import json
import os
import sys
import time
import subprocess
from pathlib import Path
from datetime import datetime

SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(SCRIPT_DIR))

class FixedPipelineOrchestrator:
    """Fixed orchestrator - skips Foundation-Sec and Claude"""
    
    def __init__(self, target_repo: str, output_dir: str = None):
        self.target_repo = Path(target_repo).resolve()
        self.output_dir = Path(output_dir or self.target_repo / ".argus" / "full-scan-fixed")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.results = {
            "scan_start": datetime.now().isoformat(),
            "target": str(self.target_repo),
            "components": {},
            "findings": [],
            "metrics": {
                "total_findings": 0,
                "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0},
                "by_tool": {},
                "total_cost_usd": 0.0,
                "total_duration_seconds": 0
            }
        }
        
    def run_scanner(self, name: str, command: list, output_file: str = None):
        """Run a security scanner"""
        print(f"\nðŸ” Running {name}...")
        start = time.time()
        
        try:
            if output_file:
                command = command + ["--json", "-o", str(self.output_dir / output_file)]
            
            result = subprocess.run(
                command,
                cwd=self.target_repo,
                capture_output=True,
                text=True,
                timeout=300
            )
            duration = time.time() - start
            
            success = result.returncode == 0
            self.results["components"][name] = {
                "status": "success" if success else "completed_with_findings",
                "duration_seconds": duration
            }
            
            print(f"âœ… {name} complete ({duration:.1f}s)")
            return result, success
            
        except Exception as e:
            duration = time.time() - start
            print(f"âš ï¸  {name} error: {e}")
            self.results["components"][name] = {
                "status": "error",
                "duration_seconds": duration,
                "error": str(e)
            }
            return None, False
    
    def component_1_deterministic_scanners(self):
        """Run all 5 deterministic scanners"""
        print("\n" + "="*80)
        print("ðŸ“Š COMPONENT 1: DETERMINISTIC SCANNERS")
        print("="*80)
        
        # TruffleHog
        self.run_scanner(
            "TruffleHog",
            ["trufflehog", "filesystem", ".", "--json"],
            "trufflehog.json"
        )
        
        # Gitleaks
        self.run_scanner(
            "Gitleaks",
            ["gitleaks", "detect", "--source", ".", "--report-format", "json", 
             "--report-path", str(self.output_dir / "gitleaks.json")],
            None
        )
        
        # Semgrep
        result, _ = self.run_scanner(
            "Semgrep",
            ["semgrep", "--config=auto", "--json", ".", "--output",
             str(self.output_dir / "semgrep.json")],
            None
        )
        
        # Trivy
        self.run_scanner(
            "Trivy",
            ["trivy", "fs", "--format", "json", ".", "--output",
             str(self.output_dir / "trivy.json")],
            None
        )
        
        # Checkov
        self.run_scanner(
            "Checkov",
            ["checkov", "-d", ".", "--output", "json", "--output-file",
             str(self.output_dir / "checkov.json")],
            None
        )
    
    def component_2_parse_and_normalize(self):
        """Parse scanner outputs and normalize"""
        print("\n" + "="*80)
        print("ðŸ”„ COMPONENT 2: PARSING & NORMALIZATION")
        print("="*80)
        
        all_findings = []
        
        # Parse Semgrep
        semgrep_file = self.output_dir / "semgrep.json"
        if semgrep_file.exists():
            try:
                with open(semgrep_file) as f:
                    data = json.load(f)
                    for result in data.get("results", []):
                        all_findings.append({
                            "tool": "semgrep",
                            "severity": result.get("extra", {}).get("severity", "medium"),
                            "title": result.get("check_id", "Unknown"),
                            "file": result.get("path", ""),
                            "line": result.get("start", {}).get("line", 0),
                            "message": result.get("extra", {}).get("message", ""),
                            "cwe": result.get("extra", {}).get("metadata", {}).get("cwe", [])
                        })
                print(f"âœ… Parsed {len(data.get('results', []))} Semgrep findings")
            except Exception as e:
                print(f"âš ï¸  Error parsing Semgrep: {e}")
        
        # Parse Trivy
        trivy_file = self.output_dir / "trivy.json"
        if trivy_file.exists():
            try:
                with open(trivy_file) as f:
                    data = json.load(f)
                    for result in data.get("Results", []):
                        for vuln in result.get("Vulnerabilities", []):
                            all_findings.append({
                                "tool": "trivy",
                                "severity": vuln.get("Severity", "medium").lower(),
                                "title": vuln.get("VulnerabilityID", "Unknown"),
                                "file": result.get("Target", ""),
                                "line": 0,
                                "message": vuln.get("Title", ""),
                                "cve": vuln.get("VulnerabilityID", ""),
                                "cvss": vuln.get("CVSS", {})
                            })
                print(f"âœ… Parsed Trivy findings")
            except Exception as e:
                print(f"âš ï¸  Error parsing Trivy: {e}")
        
        # Parse TruffleHog
        trufflehog_file = self.output_dir / "trufflehog.json"
        if trufflehog_file.exists():
            try:
                with open(trufflehog_file) as f:
                    for line in f:
                        try:
                            data = json.loads(line)
                            all_findings.append({
                                "tool": "trufflehog",
                                "severity": "critical",
                                "title": f"Secret: {data.get('DetectorName', 'Unknown')}",
                                "file": data.get("SourceMetadata", {}).get("Data", {}).get("Filesystem", {}).get("file", ""),
                                "line": 0,
                                "message": f"Detected {data.get('DetectorName', 'secret')}",
                                "verified": data.get("Verified", False)
                            })
                        except:
                            pass
                print(f"âœ… Parsed TruffleHog findings")
            except Exception as e:
                print(f"âš ï¸  Error parsing TruffleHog: {e}")
        
        # Parse Gitleaks
        gitleaks_file = self.output_dir / "gitleaks.json"
        if gitleaks_file.exists():
            try:
                with open(gitleaks_file) as f:
                    data = json.load(f)
                    for finding in data:
                        all_findings.append({
                            "tool": "gitleaks",
                            "severity": "critical",
                            "title": f"Secret: {finding.get('RuleID', 'Unknown')}",
                            "file": finding.get("File", ""),
                            "line": finding.get("StartLine", 0),
                            "message": finding.get("Description", ""),
                            "secret": finding.get("Secret", "")[:20] + "..."
                        })
                print(f"âœ… Parsed {len(data)} Gitleaks findings")
            except Exception as e:
                print(f"âš ï¸  Error parsing Gitleaks: {e}")
        
        # Parse Checkov
        checkov_file = self.output_dir / "checkov.json"
        if checkov_file.exists():
            try:
                with open(checkov_file) as f:
                    data = json.load(f)
                    for result in data.get("results", {}).get("failed_checks", []):
                        all_findings.append({
                            "tool": "checkov",
                            "severity": result.get("check_class", "").lower().replace("checkov.common.checks.", ""),
                            "title": result.get("check_name", "Unknown"),
                            "file": result.get("file_path", ""),
                            "line": result.get("file_line_range", [0])[0],
                            "message": result.get("guideline", "")
                        })
                print(f"âœ… Parsed Checkov findings")
            except Exception as e:
                print(f"âš ï¸  Error parsing Checkov: {e}")
        
        self.results["findings"] = all_findings
        self.results["metrics"]["total_findings"] = len(all_findings)
        
        # Count by severity
        for finding in all_findings:
            severity = finding.get("severity", "low")
            if severity in self.results["metrics"]["by_severity"]:
                self.results["metrics"]["by_severity"][severity] += 1
            
            tool = finding.get("tool", "unknown")
            self.results["metrics"]["by_tool"][tool] = self.results["metrics"]["by_tool"].get(tool, 0) + 1
        
        # Save normalized findings
        normalized_file = self.output_dir / "normalized_findings.json"
        with open(normalized_file, "w") as f:
            json.dump({"findings": all_findings}, f, indent=2)
        
        print(f"\nâœ… Total findings: {len(all_findings)}")
        print(f"   Critical: {self.results['metrics']['by_severity']['critical']}")
        print(f"   High: {self.results['metrics']['by_severity']['high']}")
        print(f"   Medium: {self.results['metrics']['by_severity']['medium']}")
        print(f"   Low: {self.results['metrics']['by_severity']['low']}")
    
    def component_3_cursor_ai_analysis(self):
        """Run Cursor AI analysis (skip Foundation-Sec and Claude)"""
        print("\n" + "="*80)
        print("ðŸ§  COMPONENT 3: CURSOR AI ANALYSIS")
        print("="*80)
        print("â„¹ï¸  Skipping Foundation-Sec and Claude as requested")
        print("â„¹ï¸  Using Cursor AI for code analysis")
        
        # Note: Cursor AI analysis would be done through the IDE
        # For now, we'll use the enhanced audit with best practices
        print("\nðŸ’¡ To use Cursor AI:")
        print("   1. Open the repository in Cursor IDE")
        print("   2. Use Cursor's AI features to review the findings")
        print("   3. Ask Cursor to analyze the normalized_findings.json")
        
        self.results["components"]["Cursor-AI"] = {
            "status": "manual_review_required",
            "note": "Use Cursor IDE for AI-powered analysis"
        }
    
    def run_full_pipeline(self):
        """Run the fixed pipeline"""
        print("\n" + "="*80)
        print("ðŸš€ FIXED ARGUS PIPELINE")
        print("="*80)
        print(f"Target: {self.target_repo}")
        print(f"Output: {self.output_dir}")
        print("Skipping: Foundation-Sec, Claude")
        print("Using: Cursor AI (manual)")
        print("="*80)
        
        pipeline_start = time.time()
        
        try:
            # Run components
            self.component_1_deterministic_scanners()
            self.component_2_parse_and_normalize()
            self.component_3_cursor_ai_analysis()
            
            # Finalize
            self.results["scan_end"] = datetime.now().isoformat()
            self.results["total_pipeline_duration"] = time.time() - pipeline_start
            
            # Save results
            results_file = self.output_dir / "pipeline_results.json"
            with open(results_file, "w") as f:
                json.dump(self.results, f, indent=2)
            
            # Generate summary report
            self.generate_summary_report()
            
            print("\n" + "="*80)
            print("âœ… PIPELINE COMPLETE")
            print("="*80)
            print(f"ðŸ“Š Results: {results_file}")
            print(f"â±ï¸  Duration: {self.results['total_pipeline_duration']:.1f}s")
            print(f"ðŸ” Findings: {self.results['metrics']['total_findings']}")
            print(f"   Critical: {self.results['metrics']['by_severity']['critical']}")
            print(f"   High: {self.results['metrics']['by_severity']['high']}")
            print(f"   Medium: {self.results['metrics']['by_severity']['medium']}")
            print(f"   Low: {self.results['metrics']['by_severity']['low']}")
            
            return 0
            
        except Exception as e:
            print(f"\nâŒ Pipeline error: {e}")
            import traceback
            traceback.print_exc()
            return 1
    
    def generate_summary_report(self):
        """Generate a human-readable summary"""
        report_file = self.output_dir / "SUMMARY_REPORT.md"
        
        with open(report_file, "w") as f:
            f.write("# Security Scan Summary Report\n\n")
            f.write(f"**Scan Date**: {self.results['scan_start']}\n")
            f.write(f"**Target**: {self.results['target']}\n")
            f.write(f"**Duration**: {self.results.get('total_pipeline_duration', 0):.1f}s\n\n")
            
            f.write("## Findings Summary\n\n")
            f.write(f"**Total Findings**: {self.results['metrics']['total_findings']}\n\n")
            f.write(f"- ðŸ”´ Critical: {self.results['metrics']['by_severity']['critical']}\n")
            f.write(f"- ðŸŸ  High: {self.results['metrics']['by_severity']['high']}\n")
            f.write(f"- ðŸŸ¡ Medium: {self.results['metrics']['by_severity']['medium']}\n")
            f.write(f"- âšª Low: {self.results['metrics']['by_severity']['low']}\n\n")
            
            f.write("## By Tool\n\n")
            for tool, count in self.results['metrics']['by_tool'].items():
                f.write(f"- {tool}: {count} findings\n")
            
            f.write("\n## Critical & High Findings\n\n")
            for finding in self.results['findings']:
                if finding.get('severity') in ['critical', 'high']:
                    f.write(f"### [{finding.get('severity', 'unknown').upper()}] {finding.get('title', 'Unknown')}\n")
                    f.write(f"- **Tool**: {finding.get('tool', 'unknown')}\n")
                    f.write(f"- **File**: `{finding.get('file', 'unknown')}`\n")
                    if finding.get('line', 0) > 0:
                        f.write(f"- **Line**: {finding.get('line')}\n")
                    f.write(f"- **Message**: {finding.get('message', 'No description')}\n\n")
            
            f.write("\n## Next Steps\n\n")
            f.write("1. Review critical and high severity findings\n")
            f.write("2. Use Cursor AI to analyze findings in context\n")
            f.write("3. Prioritize fixes based on exploitability\n")
            f.write("4. Re-run scan after fixes\n")
        
        print(f"ðŸ“„ Summary report: {report_file}")


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 run_full_pipeline_fixed.py <target_repo>")
        sys.exit(1)
    
    target_repo = sys.argv[1]
    orchestrator = FixedPipelineOrchestrator(target_repo)
    sys.exit(orchestrator.run_full_pipeline())


if __name__ == "__main__":
    main()

