#!/usr/bin/env python3
"""
LLM Exploitability Triage - Phase 2.2
Uses Foundation-Sec-8B for fast exploitability assessment
"""

import json
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from normalizer.base import Finding
from providers.sagemaker_foundation_sec import SageMakerFoundationSecProvider


class ExploitabilityTriage:
    """
    Fast exploitability assessment using Foundation-Sec-8B

    Classifies vulnerabilities as:
    - trivial: Exploitable with public tools/PoCs
    - moderate: Requires some skill/custom tooling
    - complex: Requires significant expertise
    - theoretical: Not practically exploitable
    """

    def __init__(self):
        self.foundation_sec = SageMakerFoundationSecProvider()
        print("âœ… Foundation-Sec-8B initialized for exploitability triage")

    def triage_findings(self, findings: list[Finding]) -> list[Finding]:
        """
        Assess exploitability for all findings

        Returns:
            List of findings with updated exploitability field
        """
        print(f"\nðŸŽ¯ Triaging exploitability for {len(findings)} findings...")

        # Only triage findings that need it
        needs_triage = [f for f in findings if f.exploitability == "unknown" and f.severity in ["high", "critical"]]

        print(f"   {len(needs_triage)} findings need exploitability assessment")

        for i, finding in enumerate(needs_triage, 1):
            print(f"   [{i}/{len(needs_triage)}] Assessing {finding.rule_name}...")

            try:
                # Get exploitability assessment from Foundation-Sec
                assessment = self._assess_exploitability(finding)

                # Update finding
                finding.exploitability = assessment["level"]
                finding.evidence["exploitability_reasoning"] = assessment["reasoning"]
                finding.evidence["exploit_prerequisites"] = assessment.get("prerequisites", [])
                finding.evidence["exploit_impact"] = assessment.get("impact", "")

                # Recalculate risk score with new exploitability
                finding.risk_score = finding.calculate_risk_score()

            except Exception as e:
                print(f"      âš ï¸  Assessment failed: {e}")
                finding.exploitability = "unknown"

        print("\nâœ… Exploitability triage complete")
        print(f"   Trivial: {sum(1 for f in findings if f.exploitability == 'trivial')}")
        print(f"   Moderate: {sum(1 for f in findings if f.exploitability == 'moderate')}")
        print(f"   Complex: {sum(1 for f in findings if f.exploitability == 'complex')}")
        print(f"   Theoretical: {sum(1 for f in findings if f.exploitability == 'theoretical')}")

        return findings

    def _assess_exploitability(self, finding: Finding) -> dict[str, any]:
        """
        Use Foundation-Sec to assess exploitability

        Returns dict with:
        - level: trivial/moderate/complex/theoretical
        - reasoning: why this classification
        - prerequisites: what attacker needs
        - impact: what attacker gains
        """
        # Build assessment prompt
        prompt = self._build_exploit_assessment_prompt(finding)

        # Get assessment from Foundation-Sec
        response = self.foundation_sec.analyze_code(
            code=finding.evidence.get("snippet", ""), context=prompt, focus="exploitability_assessment"
        )

        # Parse response
        assessment = self._parse_exploit_assessment(response)

        return assessment

    def _build_exploit_assessment_prompt(self, finding: Finding) -> str:
        """Build prompt for Foundation-Sec exploitability assessment"""

        # Gather context
        context_parts = []

        # Basic finding info
        context_parts.append(f"**Vulnerability:** {finding.rule_name}")
        context_parts.append(f"**Category:** {finding.category}")
        context_parts.append(f"**Severity:** {finding.severity}")

        if finding.cve:
            context_parts.append(f"**CVE:** {finding.cve}")
        if finding.cwe:
            context_parts.append(f"**CWE:** {finding.cwe}")
        if finding.cvss:
            context_parts.append(f"**CVSS:** {finding.cvss}")

        # Location
        context_parts.append(f"**Location:** {finding.path}:{finding.line}")
        context_parts.append(f"**Service Tier:** {finding.business_context.get('exposure', 'internal')}")

        # Evidence
        if finding.evidence.get("message"):
            context_parts.append(f"**Evidence:** {finding.evidence['message']}")

        # Code snippet
        if finding.evidence.get("snippet"):
            context_parts.append(f"\n**Code:**\n```\n{finding.evidence['snippet']}\n```")

        context = "\n".join(context_parts)

        # Build full prompt
        prompt = f"""Assess the exploitability of this security vulnerability.

{context}

**Task:** Classify the exploitability level and provide analysis.

**Exploitability Levels:**
- **trivial**: Exploitable with public tools/PoCs, no special skills required
- **moderate**: Requires some security knowledge and custom tooling
- **complex**: Requires significant expertise, custom exploit development
- **theoretical**: Not practically exploitable (requires impossible conditions)

**Consider:**
1. Are there public exploits or PoCs available?
2. What prerequisites does an attacker need (auth, network access, etc.)?
3. How complex is the exploit chain?
4. What mitigating factors exist?
5. What is the realistic impact?

Respond with ONLY a JSON object:
{{
  "level": "trivial|moderate|complex|theoretical",
  "reasoning": "2-3 sentence explanation",
  "prerequisites": ["list", "of", "requirements"],
  "impact": "what attacker gains",
  "confidence": 0.0-1.0
}}
"""

        return prompt

    def _parse_exploit_assessment(self, response: str) -> dict[str, any]:
        """Parse Foundation-Sec exploitability assessment"""
        try:
            # Extract JSON from response
            if "{" in response and "}" in response:
                start = response.index("{")
                end = response.rindex("}") + 1
                json_str = response[start:end]
                assessment = json.loads(json_str)

                # Validate level
                valid_levels = ["trivial", "moderate", "complex", "theoretical"]
                if assessment.get("level") not in valid_levels:
                    assessment["level"] = "moderate"  # Default

                return assessment
        except Exception as e:
            print(f"      âš ï¸  Failed to parse assessment: {e}")

        # Fallback: parse from text
        response_lower = response.lower()

        if "trivial" in response_lower or "easily exploitable" in response_lower:
            level = "trivial"
        elif "complex" in response_lower or "difficult" in response_lower:
            level = "complex"
        elif "theoretical" in response_lower or "not exploitable" in response_lower:
            level = "theoretical"
        else:
            level = "moderate"

        return {
            "level": level,
            "reasoning": response[:200],  # First 200 chars
            "prerequisites": [],
            "impact": "Unknown",
            "confidence": 0.5,
        }

    def batch_triage(self, findings: list[Finding], batch_size: int = 10) -> list[Finding]:
        """
        Batch process findings for efficiency

        Groups similar findings and processes them together
        """
        print(f"\nðŸŽ¯ Batch triaging {len(findings)} findings...")

        # Group by rule_id for batch processing
        by_rule = {}
        for f in findings:
            if f.exploitability == "unknown" and f.severity in ["high", "critical"]:
                if f.rule_id not in by_rule:
                    by_rule[f.rule_id] = []
                by_rule[f.rule_id].append(f)

        # Process each rule group
        for rule_id, rule_findings in by_rule.items():
            print(f"   Processing {len(rule_findings)} instances of {rule_id}...")

            # Assess first finding in detail
            if rule_findings:
                first_assessment = self._assess_exploitability(rule_findings[0])
                rule_findings[0].exploitability = first_assessment["level"]
                rule_findings[0].evidence["exploitability_reasoning"] = first_assessment["reasoning"]

                # Apply same assessment to similar findings (with lower confidence)
                for finding in rule_findings[1:]:
                    finding.exploitability = first_assessment["level"]
                    finding.evidence["exploitability_reasoning"] = (
                        f"Similar to {rule_findings[0].id[:8]}: {first_assessment['reasoning']}"
                    )
                    finding.confidence *= 0.9  # Slightly lower confidence for batch-applied

                # Recalculate risk scores
                for finding in rule_findings:
                    finding.risk_score = finding.calculate_risk_score()

        print("âœ… Batch triage complete")

        return findings


def main():
    """CLI interface for exploitability triage"""
    import argparse

    parser = argparse.ArgumentParser(description="Triage exploitability of security findings")
    parser.add_argument("--input", "-i", required=True, help="Input findings JSON file")
    parser.add_argument("--output", "-o", required=True, help="Output triaged findings JSON file")
    parser.add_argument("--batch", action="store_true", help="Use batch mode for efficiency")

    args = parser.parse_args()

    # Load findings
    with open(args.input) as f:
        findings_data = json.load(f)

    findings = [Finding.from_dict(f) for f in findings_data]

    # Triage exploitability
    triage = ExploitabilityTriage()

    triaged_findings = triage.batch_triage(findings) if args.batch else triage.triage_findings(findings)

    # Save results
    with open(args.output, "w") as f:
        json.dump([f.to_dict() for f in triaged_findings], f, indent=2)

    print(f"\nâœ… Triaged findings saved to {args.output}")


if __name__ == "__main__":
    main()
